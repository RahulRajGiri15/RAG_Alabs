{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG Pipeline - Analysis & Testing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## loading the documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "\n",
                "from langchain_community.document_loaders import PyPDFLoader\n",
                "\n",
                "DATA_DIR = os.path.join('..', 'data')\n",
                "pdf_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.pdf')]\n",
                "\n",
                "print(f\"found {len(pdf_files)} PDFs: {pdf_files}\")\n",
                "\n",
                "all_docs = []\n",
                "for pdf in pdf_files:\n",
                "    loader = PyPDFLoader(os.path.join(DATA_DIR, pdf))\n",
                "    docs = loader.load()\n",
                "    all_docs.extend(docs)\n",
                "    total_words = sum(len(d.page_content.split()) for d in docs)\n",
                "    print(f\"{pdf}: {len(docs)} pages, {total_words} words\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## chunk analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "CHUNKS_PATH = os.path.join('..', 'chunks', 'chunks.json')\n",
                "\n",
                "with open(CHUNKS_PATH, 'r', encoding='utf-8') as f:\n",
                "    chunks = json.load(f)\n",
                "\n",
                "print(f\"total chunks: {len(chunks)}\")\n",
                "\n",
                "word_counts = [c['word_count'] for c in chunks]\n",
                "\n",
                "print(f\"min words: {min(word_counts)}\")\n",
                "print(f\"max words: {max(word_counts)}\")\n",
                "print(f\"avg words: {sum(word_counts) / len(word_counts):.1f}\")\n",
                "print(f\"median: {sorted(word_counts)[len(word_counts)//2]}\")\n",
                "\n",
                "# distribution\n",
                "buckets = {'<50': 0, '50-100': 0, '100-150': 0, '150-200': 0, '200-300': 0, '>300': 0}\n",
                "for wc in word_counts:\n",
                "    if wc < 50: buckets['<50'] += 1\n",
                "    elif wc < 100: buckets['50-100'] += 1\n",
                "    elif wc < 150: buckets['100-150'] += 1\n",
                "    elif wc < 200: buckets['150-200'] += 1\n",
                "    elif wc < 300: buckets['200-300'] += 1\n",
                "    else: buckets['>300'] += 1\n",
                "\n",
                "print(\"\\nword count distribution:\")\n",
                "for bucket, count in buckets.items():\n",
                "    print(f\"  {bucket}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# look at a few example chunks\n",
                "for i in [0, len(chunks)//2, len(chunks)-1]:\n",
                "    c = chunks[i]\n",
                "    print(f\"\\nchunk {c['chunk_id']} (page {c['metadata'].get('page', '?')}, {c['word_count']} words):\")\n",
                "    print(c['content'][:250] + '...')\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## embeddings and vector store"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.embedder import get_embedding_model\n",
                "from src.vector_store import get_vector_store, get_chunk_count\n",
                "from src.config import EMBEDDING_MODEL, CHROMA_COLLECTION, VECTORDB_DIR\n",
                "\n",
                "print(f\"model: {EMBEDDING_MODEL}\")\n",
                "\n",
                "embed_model = get_embedding_model()\n",
                "sample = embed_model.embed_query(\"test query\")\n",
                "print(f\"embedding dim: {len(sample)}\")\n",
                "print(f\"first 5 values: {sample[:5]}\")\n",
                "\n",
                "print(f\"\\nchromadb collection: {CHROMA_COLLECTION}\")\n",
                "print(f\"indexed chunks: {get_chunk_count()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## testing retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.retriever import retrieve_documents\n",
                "\n",
                "queries = [\n",
                "    \"What are eBay's seller fees?\",\n",
                "    \"What happens if my account is suspended?\",\n",
                "    \"How does eBay handle intellectual property disputes?\",\n",
                "    \"What is the arbitration agreement?\",\n",
                "    \"Can I sell vehicles on eBay?\",\n",
                "]\n",
                "\n",
                "for q in queries:\n",
                "    print(f\"\\nquery: {q}\")\n",
                "    results = retrieve_documents(q, top_k=3)\n",
                "    for i, doc in enumerate(results, 1):\n",
                "        page = doc['metadata'].get('page', '?')\n",
                "        print(f\"  [{i}] score={doc['score']:.4f}, page {page}: {doc['content'][:100]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## full pipeline test (non-streaming)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.rag_pipeline import format_context\n",
                "from src.generator import generate_response\n",
                "\n",
                "test_q = \"What are the fees charged to sellers on eBay?\"\n",
                "\n",
                "docs = retrieve_documents(test_q, top_k=5)\n",
                "print(f\"got {len(docs)} chunks, scores: {docs[0]['score']:.4f} to {docs[-1]['score']:.4f}\")\n",
                "\n",
                "context = format_context(docs)\n",
                "print(f\"context: {len(context)} chars\\n\")\n",
                "\n",
                "response = generate_response(test_q, context)\n",
                "print(\"response:\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# this should fail gracefully - question is not about the document\n",
                "bad_q = \"What is the weather in New York today?\"\n",
                "docs = retrieve_documents(bad_q, top_k=3)\n",
                "print(f\"query: {bad_q}\")\n",
                "print(f\"scores: {[d['score'] for d in docs]}\")\n",
                "\n",
                "context = format_context(docs)\n",
                "response = generate_response(bad_q, context)\n",
                "print(f\"\\nresponse: {response}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}